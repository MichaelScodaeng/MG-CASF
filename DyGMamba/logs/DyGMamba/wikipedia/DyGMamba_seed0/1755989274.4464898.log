2025-08-24 07:47:54,449 - root - INFO - ********** Run 1 starts. **********
2025-08-24 07:47:54,449 - root - INFO - configuration is Namespace(dataset_name='wikipedia', batch_size=200, model_name='DyGMamba', gpu=0, num_neighbors=20, sample_neighbor_strategy='uniform', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=2, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, gamma=0.5, num_epochs=100, optimizer='Adam', weight_decay=0.0, patience=20, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=1, negative_sample_strategy='random', max_interaction_times=10, load_best_configs=False, fusion_strategy='use', spatial_dim=64, temporal_dim=64, ccasf_output_dim=128, use_integrated_mpgnn=True, use_sequential_fallback=False, embedding_mode='spatiotemporal_only', enable_base_embedding=False, rpearl_hidden=64, rpearl_mlp_layers=2, rpearl_k=16, lete_hidden=64, lete_layers=2, lete_p=0.5, use_hidden_dim=128, use_num_casm_layers=3, use_num_smpn_layers=3, caga_hidden_dim=128, caga_num_heads=8, clifford_dim=4, clifford_signature='euclidean', use_memory=False, memory_dim=128, output_dim=128, mamba_d_model=128, mamba_d_state=16, mamba_d_conv=4, mamba_expand=2, enable_feature_caching=True, clear_cache_interval=100, n_degree=20, n_head=2, n_layer=2, drop_out=0.1, lr=0.0001, n_epoch=100, seed=0, uniform=False, different_new_nodes=False, device='cuda:0', save_model_name='DyGMamba_seed0')
2025-08-24 07:47:54,449 - root - INFO - 🧠 Using INTEGRATED MPGNN approach (theoretical compliance)
2025-08-24 07:47:54,467 - root - WARNING - ❌ Failed to create integrated model: 'int' object is not callable
2025-08-24 07:47:54,468 - root - INFO - 🔄 Falling back to sequential approach...
2025-08-24 07:47:54,468 - root - WARNING - ⚠️  Using SEQUENTIAL approach (non-theoretical fallback)
2025-08-24 07:47:54,468 - root - WARNING - ⚠️  Enhanced features computed AFTER message passing
2025-08-24 07:47:55,885 - root - INFO - Model approach: 🔄 SEQUENTIAL (non-theoretical)
2025-08-24 07:47:55,885 - root - INFO - model -> Sequential(
  (0): DyGMamba(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (neighbor_co_occurrence_encoder): NIFEncoder(
      (nif_encode_layer): Sequential(
        (0): Linear(in_features=1, out_features=50, bias=True)
        (1): ReLU()
        (2): Linear(in_features=50, out_features=50, bias=True)
      )
    )
    (projection_layer): ModuleDict(
      (node): Linear(in_features=172, out_features=50, bias=True)
      (edge): Linear(in_features=172, out_features=50, bias=True)
      (time): Linear(in_features=100, out_features=50, bias=True)
      (neighbor_co_occurrence): Linear(in_features=50, out_features=50, bias=True)
    )
    (output_layer): Linear(in_features=100, out_features=172, bias=True)
    (output_layer_t_diff): Linear(in_features=25, out_features=172, bias=True)
    (mamba): ModuleList(
      (0-1): 2 x Mamba(
        (in_proj): Linear(in_features=100, out_features=200, bias=False)
        (conv1d): Conv1d(100, 100, kernel_size=(4,), stride=(1,), padding=(3,), groups=100)
        (act): SiLU()
        (x_proj): Linear(in_features=100, out_features=39, bias=False)
        (dt_proj): Linear(in_features=7, out_features=100, bias=True)
        (out_proj): Linear(in_features=100, out_features=100, bias=False)
      )
    )
    (mamba_t_diff): ModuleList(
      (0-1): 2 x Mamba(
        (in_proj): Linear(in_features=25, out_features=50, bias=False)
        (conv1d): Conv1d(25, 25, kernel_size=(4,), stride=(1,), padding=(3,), groups=25)
        (act): SiLU()
        (x_proj): Linear(in_features=25, out_features=34, bias=False)
        (dt_proj): Linear(in_features=2, out_features=25, bias=True)
        (out_proj): Linear(in_features=25, out_features=25, bias=False)
      )
    )
    (projection_layer_t_diff): Linear(in_features=100, out_features=25, bias=True)
    (projection_layer_t_diff_up): Linear(in_features=25, out_features=100, bias=True)
    (weightagg): Linear(in_features=100, out_features=1, bias=True)
    (reduce_layer): Linear(in_features=200, out_features=100, bias=True)
    (channel_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (channel_feedforward): FeedForwardNet(
      (ffn): Sequential(
        (0): Linear(in_features=100, out_features=400, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=400, out_features=100, bias=True)
        (4): Dropout(p=0.1, inplace=False)
      )
    )
    (neighbor_selection_layer): Linear(in_features=100, out_features=100, bias=True)
  )
  (1): MergeLayerTD(
    (fc1): Linear(in_features=516, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
2025-08-24 07:47:55,886 - root - INFO - model name: DyGMamba, #parameters: 1341268 B, 1309.83203125 KB, 1.2791328430175781 MB.
