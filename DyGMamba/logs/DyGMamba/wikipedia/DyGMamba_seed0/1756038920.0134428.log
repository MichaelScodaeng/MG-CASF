2025-08-24 21:35:20,018 - root - INFO - ********** Run 1 starts. **********
2025-08-24 21:35:20,018 - root - INFO - configuration is Namespace(dataset_name='wikipedia', batch_size=200, model_name='DyGMamba', gpu=0, num_neighbors=20, sample_neighbor_strategy='uniform', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=2, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, time_window_mode='fixed_proportion', patch_size=1, channel_embedding_dim=50, max_input_sequence_length=32, learning_rate=0.0001, dropout=0.1, gamma=0.5, num_epochs=100, optimizer='Adam', weight_decay=0.0, patience=20, val_ratio=0.15, test_ratio=0.15, num_runs=1, test_interval_epochs=1, negative_sample_strategy='random', max_interaction_times=10, load_best_configs=False, fusion_strategy='use', spatial_dim=64, temporal_dim=64, ccasf_output_dim=128, use_integrated_mpgnn=True, use_sequential_fallback=False, embedding_mode='spatiotemporal_only', enable_base_embedding=False, rpearl_hidden=64, rpearl_mlp_layers=2, rpearl_k=16, lete_hidden=64, lete_layers=2, lete_p=0.5, use_hidden_dim=128, use_num_casm_layers=3, use_num_smpn_layers=3, caga_hidden_dim=128, caga_num_heads=8, clifford_dim=4, clifford_signature='euclidean', use_memory=False, memory_dim=128, output_dim=128, mamba_d_model=128, mamba_d_state=16, mamba_d_conv=4, mamba_expand=2, enable_feature_caching=True, clear_cache_interval=100, n_degree=20, n_head=2, n_layer=2, drop_out=0.1, lr=0.0001, n_epoch=100, seed=0, uniform=False, different_new_nodes=False, device='cuda:0', save_model_name='DyGMamba_seed0')
2025-08-24 21:35:20,018 - root - INFO - ðŸ§  Using INTEGRATED MPGNN approach (theoretical compliance)
2025-08-24 21:35:29,644 - root - INFO - âœ… Integrated DyGMamba created successfully
2025-08-24 21:35:29,644 - root - INFO -    Embedding mode: spatiotemporal_only
2025-08-24 21:35:29,644 - root - INFO -    Base embedding: Disabled
2025-08-24 21:35:29,644 - root - INFO -    Fusion strategy: use
2025-08-24 21:35:29,644 - root - INFO -    Theoretical compliance: MPGNN âœ“
2025-08-24 21:35:29,645 - root - INFO - Model approach: ðŸ§  INTEGRATED MPGNN (theoretical)
2025-08-24 21:35:29,645 - root - INFO - model -> Sequential(
  (0): IntegratedDyGMamba(
    (enhanced_feature_manager): EnhancedNodeFeatureManager(
      (spatial_generator): TrainableSpatialGenerator(
        (spatial_encoder): Sequential(
          (spatial_layer_0): Sequential(
            (0): Linear(in_features=172, out_features=64, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (spatial_layer_1): Sequential(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (spatial_output): Linear(in_features=64, out_features=64, bias=True)
        (position_encoder): Embedding(16, 64)
        (time_spatial_encoder): Linear(in_features=1, out_features=64, bias=True)
      )
      (temporal_generator): TrainableTemporalGenerator(
        (time_encoder): TimeEncoder(
          (w): Linear(in_features=1, out_features=100, bias=True)
        )
        (temporal_encoder): Sequential(
          (temporal_layer_0): Sequential(
            (0): Linear(in_features=272, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (temporal_layer_1): Sequential(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
        (temporal_output): Linear(in_features=64, out_features=64, bias=True)
      )
      (fusion_module): TrainableUSEFusion(
        (casm_net): Sequential(
          (casm_layer_0): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (casm_layer_1): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (casm_layer_2): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
        )
        (casm_output): Linear(in_features=128, out_features=16, bias=True)
        (smpn): Sequential(
          (smpn_layer_0): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.1, inplace=False)
            (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (smpn_layer_1): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.1, inplace=False)
            (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (smpn_layer_2): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.1, inplace=False)
            (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
        )
        (smpn_output): Linear(in_features=128, out_features=16, bias=True)
        (final_projection): Linear(in_features=16, out_features=128, bias=True)
      )
    )
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (feature_projection): Linear(in_features=300, out_features=128, bias=True)
    (edge_projection): Linear(in_features=172, out_features=128, bias=True)
    (time_projection): Linear(in_features=100, out_features=128, bias=True)
    (mamba_layers): ModuleList(
      (0-1): 2 x IntegratedDyGMambaLayer(
        (mamba_block): Mamba(
          (in_proj): Linear(in_features=128, out_features=512, bias=False)
          (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
          (act): SiLU()
          (x_proj): Linear(in_features=256, out_features=40, bias=False)
          (dt_proj): Linear(in_features=8, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=128, bias=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (graph_conv_layers): ModuleList(
      (0-1): 2 x IntegratedGraphConvLayer(
        (message_mlp): Sequential(
          (0): Linear(in_features=512, out_features=128, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=128, bias=True)
        )
        (update_mlp): Sequential(
          (0): Linear(in_features=256, out_features=128, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=128, bias=True)
        )
        (attention): Sequential(
          (0): Linear(in_features=128, out_features=1, bias=True)
          (1): LeakyReLU(negative_slope=0.2)
        )
      )
    )
    (output_layers): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=128, out_features=128, bias=True)
    )
    (layer_norms): ModuleList(
      (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (1): MergeLayerTD(
    (fc1): Linear(in_features=516, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
2025-08-24 21:35:29,647 - root - INFO - model name: DyGMamba, #parameters: 3388396 B, 3308.98046875 KB, 3.231426239013672 MB.
